{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70573,"databundleVersionId":7743921,"sourceType":"competition"},{"sourceId":7703924,"sourceType":"datasetVersion","datasetId":4497449}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Hi everyone, I'm Abrar Jahin Niloy from Team Cristiano_Ronaldo. Here's merged My Training and Inference Notebook.\n\n#### What I actually did:\n1. Initially I tried different types of model and got a good score in Byt5 transformer. So I continued with this model.\n2. First I Pretrained \"Byt5-small\" model on a simillar type competition 'DataVerse Challenge - ITVerse 2023'  \n    I Pre-trained the model according to first prize solution notebook \n    Reference Notebook: [First Prize solution training notebook](https://www.kaggle.com/code/umongsain/training-notebook)   \n    My Pre-Train Notebook: [Pre-Trained on Dataverse](https://www.kaggle.com/code/abrarjahinniloy/byte-t5-training-on-dataverse/notebook?scriptVersionId=163318207)  \n    Basically Both notebooks are same. I just copied from the reference notebook.  \n    Pre-Trained Checkpoint Link: [Model Checkpoint Trained on Dataverse](https://www.kaggle.com/datasets/abrarjahinniloy/model-checkpoint-trained-on-dataverse/data)  \n3.Then I Fine tunned with this checkpoint on this Bhashamul competition dataset.  \n  Final Training Notebook Link: [Training Notebook See Version 2](https://www.kaggle.com/code/abrarjahinniloy/bhashamul-ipa-final-training/notebook)  \n  Final Infererce Notebook Link:[Inference Notebook See Version 19](https://www.kaggle.com/code/abrarjahinniloy/bhashamul-bengali-regional-ipa-inference-2/notebook)   \n4. I have mentioned my detailed workflow on video presentation.\n\n","metadata":{}},{"cell_type":"markdown","source":"## EDA (Exploratory Data Analysis)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ntrain_df = pd.read_csv(\"/kaggle/input/regipa/train_regipa.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/regipa/test_regipa.csv\", index_col=\"Index\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:25.454607Z","iopub.execute_input":"2024-02-29T03:44:25.454869Z","iopub.status.idle":"2024-02-29T03:44:28.010624Z","shell.execute_reply.started":"2024-02-29T03:44:25.454846Z","shell.execute_reply":"2024-02-29T03:44:28.009515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop(columns=['Index','District'])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:28.012472Z","iopub.execute_input":"2024-02-29T03:44:28.012816Z","iopub.status.idle":"2024-02-29T03:44:28.034140Z","shell.execute_reply.started":"2024-02-29T03:44:28.012788Z","shell.execute_reply":"2024-02-29T03:44:28.033177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test_df.drop(columns=['District'])\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:28.035727Z","iopub.execute_input":"2024-02-29T03:44:28.036356Z","iopub.status.idle":"2024-02-29T03:44:28.045886Z","shell.execute_reply.started":"2024-02-29T03:44:28.036329Z","shell.execute_reply":"2024-02-29T03:44:28.044693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_df.isna().value_counts())\ndisplay(test_df.isna().value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:28.048801Z","iopub.execute_input":"2024-02-29T03:44:28.049301Z","iopub.status.idle":"2024-02-29T03:44:28.080931Z","shell.execute_reply.started":"2024-02-29T03:44:28.049266Z","shell.execute_reply":"2024-02-29T03:44:28.080144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.dropna(inplace=True)\ntest_df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:28.082177Z","iopub.execute_input":"2024-02-29T03:44:28.082549Z","iopub.status.idle":"2024-02-29T03:44:28.102975Z","shell.execute_reply.started":"2024-02-29T03:44:28.082515Z","shell.execute_reply":"2024-02-29T03:44:28.101993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total number of training samples:\", len(train_df))\nprint(\"Total number of test samples:\", len(test_df))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:28.104048Z","iopub.execute_input":"2024-02-29T03:44:28.104430Z","iopub.status.idle":"2024-02-29T03:44:28.110705Z","shell.execute_reply.started":"2024-02-29T03:44:28.104396Z","shell.execute_reply":"2024-02-29T03:44:28.109779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n\ntrain_lengths = train_df[\"Contents\"].str.len()\nsns.histplot(ax=axes[0], data=train_lengths, bins=10).set(xlabel=\"Length of training text samples\")\naxes[0].axvline(train_lengths.mean(), c=\"k\", ls=\"--\", lw=2.5, label=\"Mean\")\naxes[0].legend()\n\ntest_lengths = test_df[\"Contents\"].str.len()\nsns.histplot(ax=axes[1], data=test_lengths, bins=10).set(xlabel=\"Length of test text samples\")\naxes[1].axvline(test_lengths.mean(), c=\"k\", ls=\"--\", lw=2.5, label=\"Mean\")\naxes[1].legend()\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:28.111878Z","iopub.execute_input":"2024-02-29T03:44:28.112769Z","iopub.status.idle":"2024-02-29T03:44:28.893373Z","shell.execute_reply.started":"2024-02-29T03:44:28.112732Z","shell.execute_reply":"2024-02-29T03:44:28.892176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For calculating WER (Word Error Rate)\n! pip install jiwer","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:28.895092Z","iopub.execute_input":"2024-02-29T03:44:28.895403Z","iopub.status.idle":"2024-02-29T03:44:42.629879Z","shell.execute_reply.started":"2024-02-29T03:44:28.895377Z","shell.execute_reply":"2024-02-29T03:44:42.628944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\n**I used a 10% validation dataset to train the model and at 17000 steps, I achieved a respectable score. So now I will train using nearly the entire training data set while maintaining the same model parameters. **","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.05, shuffle=True, random_state=3000)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:42.631554Z","iopub.execute_input":"2024-02-29T03:44:42.632443Z","iopub.status.idle":"2024-02-29T03:44:42.802144Z","shell.execute_reply.started":"2024-02-29T03:44:42.632405Z","shell.execute_reply":"2024-02-29T03:44:42.801080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\nds_train = Dataset.from_pandas(train_df)\nds_eval = Dataset.from_pandas(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:42.805335Z","iopub.execute_input":"2024-02-29T03:44:42.806011Z","iopub.status.idle":"2024-02-29T03:44:44.058261Z","shell.execute_reply.started":"2024-02-29T03:44:42.805982Z","shell.execute_reply":"2024-02-29T03:44:44.057278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\n**For this dataset, I primarily train with 'google/byt5-small' Transformer. First, I train the model using the 'DataVerse Challenge - ITVerse 2023' competition dataset. Use the model checkpoint to train on this dataset. **","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n\nmodel_id = \"/kaggle/input/model-checkpoint-trained-on-dataverse/Model/\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)\ndata_collator = DataCollatorForSeq2Seq(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:44:44.059434Z","iopub.execute_input":"2024-02-29T03:44:44.059861Z","iopub.status.idle":"2024-02-29T03:45:22.724848Z","shell.execute_reply.started":"2024-02-29T03:44:44.059834Z","shell.execute_reply":"2024-02-29T03:45:22.724000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(sample):\n    output = tokenizer(sample[\"Contents\"])\n    output[\"labels\"] = tokenizer(sample[\"IPA\"])['input_ids']\n    output[\"length\"] = len(output[\"labels\"])\n    #print(output)\n    return output\n\n\nds_train = ds_train.map(prepare_dataset, remove_columns=ds_train.column_names)\nds_eval = ds_eval.map(prepare_dataset, remove_columns=ds_eval.column_names)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:45:22.725990Z","iopub.execute_input":"2024-02-29T03:45:22.726619Z","iopub.status.idle":"2024-02-29T03:45:37.118974Z","shell.execute_reply.started":"2024-02-29T03:45:22.726590Z","shell.execute_reply":"2024-02-29T03:45:37.118216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ds_train)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:45:37.120037Z","iopub.execute_input":"2024-02-29T03:45:37.120367Z","iopub.status.idle":"2024-02-29T03:45:37.125487Z","shell.execute_reply.started":"2024-02-29T03:45:37.120341Z","shell.execute_reply":"2024-02-29T03:45:37.124539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom datasets import load_metric\n\nwer_metric = load_metric(\"wer\")\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    \n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    result = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"wer\": result}","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:45:37.126567Z","iopub.execute_input":"2024-02-29T03:45:37.126857Z","iopub.status.idle":"2024-02-29T03:45:37.844488Z","shell.execute_reply.started":"2024-02-29T03:45:37.126829Z","shell.execute_reply":"2024-02-29T03:45:37.843461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\nmodel_id = \"bytet5-banglaregional-text-to-ipa-Final\"\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=model_id,\n    group_by_length=True,\n    length_column_name=\"length\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    evaluation_strategy=\"steps\",\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    load_best_model_at_end=False,\n    num_train_epochs=10,\n    max_steps=18000,\n    save_steps=1000,\n    eval_steps=1000,\n    logging_steps=1000,\n    learning_rate=3e-4,\n    weight_decay=1e-2,\n    warmup_steps=1000,\n    save_total_limit=2,\n    predict_with_generate=True,\n    generation_max_length=512,\n    push_to_hub=False,\n    report_to=\"none\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:45:37.845700Z","iopub.execute_input":"2024-02-29T03:45:37.845959Z","iopub.status.idle":"2024-02-29T03:45:37.926858Z","shell.execute_reply.started":"2024-02-29T03:45:37.845936Z","shell.execute_reply":"2024-02-29T03:45:37.925359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=ds_train,\n    eval_dataset=ds_eval,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T03:45:37.928345Z","iopub.execute_input":"2024-02-29T03:45:37.928983Z","iopub.status.idle":"2024-02-29T04:24:53.189283Z","shell.execute_reply.started":"2024-02-29T03:45:37.928951Z","shell.execute_reply":"2024-02-29T04:24:53.188302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T04:24:53.194060Z","iopub.execute_input":"2024-02-29T04:24:53.194372Z","iopub.status.idle":"2024-02-29T04:24:55.538224Z","shell.execute_reply.started":"2024-02-29T04:24:53.194348Z","shell.execute_reply":"2024-02-29T04:24:55.537040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2024-02-29T04:24:57.276653Z","iopub.execute_input":"2024-02-29T04:24:57.276948Z","iopub.status.idle":"2024-02-29T04:24:57.281903Z","shell.execute_reply.started":"2024-02-29T04:24:57.276923Z","shell.execute_reply":"2024-02-29T04:24:57.280745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/regipa/test_regipa.csv\", index_col=\"Index\")\ntest_df = test_df.drop(columns=['District'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\"text2text-generation\", model=\"/kaggle/working/bytet5-banglaregional-text-to-ipa-Final/checkpoint-17000/\", device=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess-1: sort by length\nindex = test_df[\"Contents\"].str.len().sort_values(ascending=False).index\ntest_df = test_df.reindex(index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess-2: removing english alphabets\nalpha_pat = \"[a-zA-Z]\"\ntest_df[\"Contents\"] = test_df[\"Contents\"].str.replace(alpha_pat, \"\", regex=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess-3: adding period at the end of those texts not having period or '।'\nindices_to_add_period = test_df[~test_df['Contents'].str.endswith('।')].index\ntest_df.loc[indices_to_add_period, 'Contents'] += '।'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntexts = test_df[\"Contents\"].tolist()\nipas = pipe(texts, max_length=512, batch_size=64)\nipas = [ipa[\"generated_text\"] for ipa in ipas]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# postprocess-1\ntest_df[\"ipa\"] = ipas\ntest_df = test_df.sort_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# postprocess-2: removing period from the end of those ipas in which texts period were added in preprocess-3\ntest_df.loc[indices_to_add_period, 'ipa'] = test_df.loc[indices_to_add_period, 'ipa'].str.rstrip('।')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.index.names = ['id']\ntest_df = test_df.drop(columns=['Contents'])\ntest_df.rename(columns={'ipa': 'string'},inplace=True)\ntest_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv(\"submission_bytebase.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}